{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# game data outputted from W1_DataCleaning.ipynb\n",
    "game_data = pd.read_csv('mydata/womens/game_data.csv')\n",
    "game_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a df for a logistic regression model to predict win probability\n",
    "\n",
    "win_prob_df = pd.DataFrame()\n",
    "\n",
    "# Response variable\n",
    "win_prob_df['Win'] = (game_data['Score_x'] > game_data['Score_y']).astype('int64')\n",
    "\n",
    "# Predictors\n",
    "\n",
    "# Difference in NCAA tournament Seeds\n",
    "win_prob_df['SeedDiff'] = game_data['Seed_x'] - game_data['Seed_y']\n",
    "\n",
    "# Difference in efficiency metrics and strength of schedule metrics\n",
    "win_prob_df['MooreRatingDiff'] = game_data['MooreRating_x'] - game_data['MooreRating_y']\n",
    "\n",
    "# season\n",
    "win_prob_df['Season'] = game_data['Season']\n",
    "\n",
    "win_prob_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make team x be the team with the higher rating and team y be the team with the lower rating\n",
    "def switch_teams(row)\n",
    "    # if rating x is less than rating y\n",
    "    if row['MooreRating_x'] < row['MooreRating_y']:\n",
    "        underdog = row['Score_x']  # \"Worse\" team's score\n",
    "        favorite = row['Score_y'# \"Better\" team's score\n",
    "        row['Score_x'] = favorite\n",
    "        row['Score_y'] = underdog\n",
    "        underdog = row['TeamID_x']  # \"Worse\" team's ID\n",
    "        favorite = row['TeamID_y']  # \"Better\" team's ID\n",
    "        row['TeamID_x'] = favorite\n",
    "        row['TeamID_y'] = underdog\n",
    "        underdog = row['MooreRating_x']  # \"Worse\" team's rating\n",
    "        favorite = row['MooreRating_y']  # \"Better\" team's rating\n",
    "        row['MooreRating_x'] = favorite\n",
    "        row['MooreRating_y'] = underdog\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_data = game_data.apply(switch_teams, axis = 1)\n",
    "game_data.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a df for a knn classifier model to predict upset probability\n",
    "\n",
    "upset_prob_df = pd.DataFrame()\n",
    "\n",
    "# Response variable\n",
    "upset_prob_df['Upset'] = (game_data['Score_x'] > game_data['Score_y']).astype('int64')\n",
    "\n",
    "# Predictors\n",
    "\n",
    "# Difference in NCAA tournament Seeds\n",
    "upset_prob_df['SeedDiff'] = game_data['Seed_x'] - game_data['Seed_y']\n",
    "\n",
    "# Difference in efficiency metrics and strength of schedule metrics\n",
    "upset_prob_df['MooreRatingDiff'] = game_data['MooreRating_x'] - game_data['MooreRating_y']\n",
    "\n",
    "# season\n",
    "upset_prob_df['Season'] = game_data['Season']\n",
    "\n",
    "upset_prob_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_coef = [1e-6, 3e-6, 1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3, 1e-2, 3e-2, 1e-1, 3e-1, 1e0, 3e0, 1e1, 3e1, 1e2, 3e2]\n",
    "\n",
    "def train_logistic_regression():\n",
    "    \n",
    "    X = win_prob_df.drop(columns = ['Win'])\n",
    "    y = win_prob_df['Win']\n",
    "    \n",
    "    # to store model info during cross validation\n",
    "    scores = {}\n",
    "    model_coefs = []\n",
    "    models = []\n",
    "    \n",
    "    # for each ridge regularization coefficient\n",
    "    for c in ridge_coef:\n",
    "        \n",
    "        # for each season\n",
    "        for season in list(X['Season'].unique()):\n",
    "\n",
    "            # add season to scores dictionary\n",
    "            if season not in scores:\n",
    "                scores[season] = []\n",
    "\n",
    "                # split into train and validation sets\n",
    "                X_train = X[X['Season'] != season].drop(columns = ['Season'])\n",
    "                X_val = X[X['Season'] == season].drop(columns = ['Season'])\n",
    "                X_train = pd.DataFrame(scale.fit_transform(X_train), columns = X_train.columns)\n",
    "                X_val = pd.DataFrame(scale.transform(X_val), columns = X_val.columns)\n",
    "                y_train = y[X_train.index]\n",
    "                y_val = y[X_val.index]\n",
    "\n",
    "                # fit logistic regression\n",
    "                log_model = LogisticRegression(penalty = 'l2', C = c, max_iter = 10000, random_state = 0, solver = \"sag\").fit(X_train, y_train)\n",
    "\n",
    "                # predict win probabilities\n",
    "                predictions = log_model.predict_proba(X_val)\n",
    "\n",
    "                # calculate log loss and store\n",
    "                val_score = log_loss(y_val, predictions)\n",
    "                scores[season].append(val_score)\n",
    "                \n",
    "        # retrain model on full dataset for coefficients\n",
    "        log_model = LogisticRegression(penalty = 'l2', C = c, max_iter = 10000, random_state = 0, solver = \"sag\").fit(pd.DataFrame(scale.fit_transform(X), columns = X.columns).drop(columns = ['Season']), y)\n",
    "\n",
    "        # store model details\n",
    "        models.append(log_model)\n",
    "        model_coefs.append({'ridge_coef': c})\n",
    "    \n",
    "    # return dataframe of results\n",
    "    return pd.DataFrame({'Type': ['log' for i in range(len(models))],\n",
    "                         'Model': models,\n",
    "                         'Model_Coef': model_coefs,\n",
    "                         '2008_Score': scores[2008],\n",
    "                         '2009_Score': scores[2009],\n",
    "                         '2010_Score': scores[2010],\n",
    "                         '2011_Score': scores[2011],\n",
    "                         '2012_Score': scores[2012],\n",
    "                         '2013_Score': scores[2013],\n",
    "                         '2014_Score': scores[2014],\n",
    "                         '2015_Score': scores[2015],\n",
    "                         '2016_Score': scores[2016],\n",
    "                         '2017_Score': scores[2017],\n",
    "                         '2018_Score': scores[2018],\n",
    "                         '2019_Score': scores[2019]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = [10 + i for i in range(99)]\n",
    "weights = ['uniform', 'distance']\n",
    "\n",
    "def train_knn():\n",
    "    \n",
    "    X = upset_prob_df.drop(columns = ['Upset'])\n",
    "    y = upset_prob_df['Upset']\n",
    "    \n",
    "    # to store model info during cross validation\n",
    "    scores = {}\n",
    "    model_coefs = []\n",
    "    models = []\n",
    "    \n",
    "    # for repeatable randomization\n",
    "    random.seed(0)\n",
    "    \n",
    "    # train 50 models\n",
    "    for i in range(50):\n",
    "        \n",
    "        knn_params = {'n_neighbors': ks[random.randint(0, len(ks) - 1)],\n",
    "                     'weights': weights[random.randint(0, len(weights) - 1)]}\n",
    "        \n",
    "        # for each season\n",
    "        for season in list(X['Season'].unique()):\n",
    "\n",
    "            # add season to scores dictionary\n",
    "            if season not in scores:\n",
    "                scores[season] = []\n",
    "\n",
    "                # split into train and validation sets\n",
    "                X_train = X[X['Season'] != season].drop(columns = ['Season'])\n",
    "                X_val = X[X['Season'] == season].drop(columns = ['Season'])\n",
    "                X_train = pd.DataFrame(scale.fit_transform(X_train), columns = X_train.columns)\n",
    "                X_val = pd.DataFrame(scale.transform(X_val), columns = X_val.columns)\n",
    "                y_train = y[X_train.index]\n",
    "                y_val = y[X_val.index]\n",
    "\n",
    "                # fit knn\n",
    "                knn_model = KNeighborsClassifier(n_neighbors = knn_params['n_neighbors'],\n",
    "                                                 weights = knn_params['weights']).fit(X_train, y_train)\n",
    "\n",
    "                # predict win probabilities\n",
    "                predictions = knn_model.predict_proba(X_val)\n",
    "\n",
    "                # calculate log loss and store\n",
    "                val_score = log_loss(y_val, predictions)\n",
    "                scores[season].append(val_score)\n",
    "                \n",
    "        # store model details, no need to store KNN model yet\n",
    "        models.append(None)\n",
    "        model_coefs.append({'k': knn_params['n_neighbors'], 'weights': knn_params['weights']})\n",
    "    \n",
    "    # return dataframe of results\n",
    "    return pd.DataFrame({'Type': ['knn' for i in range(len(models))],\n",
    "                         'Model': models,\n",
    "                         'Model_Coef': model_coefs,\n",
    "                         '2008_Score': scores[2008],\n",
    "                         '2009_Score': scores[2009],\n",
    "                         '2010_Score': scores[2010],\n",
    "                         '2011_Score': scores[2011],\n",
    "                         '2012_Score': scores[2012],\n",
    "                         '2013_Score': scores[2013],\n",
    "                         '2014_Score': scores[2014],\n",
    "                         '2015_Score': scores[2015],\n",
    "                         '2016_Score': scores[2016],\n",
    "                         '2017_Score': scores[2017],\n",
    "                         '2018_Score': scores[2018],\n",
    "                         '2019_Score': scores[2019]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the models\n",
    "log_results = train_logistic_regression()\n",
    "knn_results = train_knn()\n",
    "final_results = pd.concat([log_results, knn_results], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate percentiles of scores in relation to other models\n",
    "percentile_cols = []\n",
    "score_cols = []\n",
    "for col in final_results.columns:\n",
    "    if col.endswith(\"Score\"):\n",
    "        score_cols.append(col)\n",
    "        final_results[col[0:4] + \"_Percentile\"] = final_results[col].rank(pct = True, ascending = False)\n",
    "        percentile_cols.append(col[0:4] + \"_Percentile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average score and average percentile\n",
    "final_results['Avg_Score'] = final_results[score_cols].mean(axis = 1)\n",
    "final_results['Avg_Percentile'] = final_results[percentile_cols].mean(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to show feature importances of given model by results row index\n",
    "def features(row_index):\n",
    "    \n",
    "    # if model is logistic regression, return coefficients\n",
    "    if final_results.loc[row_index, 'Type'] == 'log':\n",
    "        \n",
    "        model = final_results.loc[row_index, 'Model']\n",
    "        \n",
    "        result_string = 'Feature Coefficients: \\n\\n'\n",
    "        \n",
    "        model_features = final_results.loc[row_index, 'Features']\n",
    "        for i in range(len(model_features)):\n",
    "            result_string = result_string + model_features[i] + ': ' + model.coef_[0][i] + '\\n'\n",
    "        print(result_string)\n",
    "        \n",
    "    # KNN doesn't really have a feature importance\n",
    "    else:\n",
    "        print('All features have equal weight in KNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results\n",
    "pd.set_option('display.max_rows', None)\n",
    "final_results[['Type', 'Model_Coef', 'Avg_Score', 'Avg_Percentile']].sort_values(by = ['Avg_Score'], ascending = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
