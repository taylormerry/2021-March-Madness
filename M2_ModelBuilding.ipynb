{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "\n",
    "Build logistic regression, random forest, and XGBoost models to predict the probability of an upset in the NCAA tournament\n",
    "\n",
    "Based on data output in MDataCleaning.ipynb, model used in MPredictions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matchup data outputted from MDataCleaning.ipynb\n",
    "matchups = pd.read_csv('mydata/matchups.csv')\n",
    "matchups.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = matchups['Upset']\n",
    "X = matchups.drop(columns = ['Upset'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lasso regularization for feature selection\n",
    "lasso_reg = [i / 1000 for i in range(1, 350, 1)]\n",
    "\n",
    "# ridge regularization to prevent overfitting\n",
    "ridge_reg = [.0003, .001, .003, .01, .03]\n",
    "\n",
    "# scale used to scale columns before applying regularization\n",
    "scale = StandardScaler()\n",
    "\n",
    "def train_logistic_regression():\n",
    "    \n",
    "    # list/dictionaries to store results\n",
    "    feature_list = []\n",
    "    scores = {}\n",
    "    model_coefs = []\n",
    "    models = []\n",
    "    num_features = []\n",
    "    \n",
    "    # for each possible lasso regularization coefficient\n",
    "    for c in tqdm(lasso_reg):\n",
    "        \n",
    "        # dont use season as a feature\n",
    "        X_lasso = X.drop(columns = ['Season'])\n",
    "        \n",
    "        # scale columns before regularization\n",
    "        X_lasso = pd.DataFrame(scale.fit_transform(X_lasso), columns = X_lasso.columns)\n",
    "        \n",
    "        # fit L1 logistic regression for feature selection\n",
    "        lasso = LogisticRegression(penalty = 'l1', C = c, random_state = 0, solver = 'saga', max_iter = 10000).fit(X_lasso, y)\n",
    "        \n",
    "        # filter for the columns with nonzero coefficients and the season column\n",
    "        zero_cols = []\n",
    "        for i in range(len(lasso.coef_[0])):\n",
    "            if lasso.coef_[0][i] == 0.0:\n",
    "                zero_cols.append(X_lasso.columns[i])\n",
    "        nonzero_X = X.drop(columns = zero_cols)\n",
    "        \n",
    "        # if there is at least 1 nonzero column and the same amount of features hasn't already been built\n",
    "        if (len(nonzero_X.columns) > 1 and (len(nonzero_X.columns) - 1) not in num_features):\n",
    "            \n",
    "            # try each L2 regularization coefficient\n",
    "            for c2 in ridge_reg:\n",
    "                \n",
    "                # cross validate over each season\n",
    "                for season in list(X['Season'].unique()):\n",
    "                    \n",
    "                    # add season to scores dictionary\n",
    "                    if season not in scores:\n",
    "                        scores[season] = []\n",
    "                        \n",
    "                    # split into train and validation sets\n",
    "                    X_train = nonzero_X[nonzero_X['Season'] != season].drop(columns = ['Season'])\n",
    "                    X_val = nonzero_X[nonzero_X['Season'] == season].drop(columns = ['Season'])\n",
    "                    X_train = pd.DataFrame(scale.fit_transform(X_train), columns = X_train.columns)\n",
    "                    X_val = pd.DataFrame(scale.transform(X_val), columns = X_val.columns)\n",
    "                    y_train = y[X_train.index]\n",
    "                    y_val = y[X_val.index]\n",
    "                    \n",
    "                    # fit logistic regression\n",
    "                    log_model = LogisticRegression(penalty = 'l2', C = c2, max_iter = 10000, random_state = 0, solver = \"sag\").fit(X_train, y_train)\n",
    "                    \n",
    "                    # predict win probabilities\n",
    "                    predictions = log_model.predict_proba(X_val)\n",
    "                    \n",
    "                    # calculate log loss and store\n",
    "                    val_score = log_loss(y_val, predictions)\n",
    "                    scores[season].append(val_score)\n",
    "                    \n",
    "                # store model details\n",
    "                feature_list.append(X_val.columns)\n",
    "                num_features.append(len(X_val.columns))\n",
    "                models.append(log_model)\n",
    "                model_coefs.append({'lasso_coef': lasso_c, 'ridge_coef': ridge_c})\n",
    "                \n",
    "    # return dataframe of results\n",
    "    return pd.DataFrame({'Type': ['log' for i in range(len(models))],\n",
    "                         'Num_Features': num_features,\n",
    "                         'Features': feature_list,\n",
    "                         'Model': models,\n",
    "                         'Model_Coef': model_coefs,\n",
    "                         '2008_Score': scores[2008],\n",
    "                         '2009_Score': scores[2009],\n",
    "                         '2010_Score': scores[2010],\n",
    "                         '2011_Score': scores[2011],\n",
    "                         '2012_Score': scores[2012],\n",
    "                         '2013_Score': scores[2013],\n",
    "                         '2014_Score': scores[2014],\n",
    "                         '2015_Score': scores[2015],\n",
    "                         '2016_Score': scores[2016],\n",
    "                         '2017_Score': scores[2017],\n",
    "                         '2018_Score': scores[2018],\n",
    "                         '2019_Score': scores[2019]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest paramaters\n",
    "max_features_list = [5, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 35, 40]\n",
    "rf_max_depths = [5, 10, 15, 20, None]\n",
    "min_samples = [1, 0.01, 0.03]\n",
    "criterions = ['gini', 'entropy']\n",
    "\n",
    "def train_random_forest():\n",
    "    \n",
    "    # list/dictionaries to store results\n",
    "    scores = {}\n",
    "    models = []\n",
    "    model_coefs = []\n",
    "    \n",
    "    random.seed(0)\n",
    "    \n",
    "    # fit 10 models\n",
    "    for i in tqdm(range(10)):\n",
    "        \n",
    "        # randomly generate random forest params\n",
    "        rf_params = {'max_features': [random.randint(0, len(max_features_list) - 1)],\n",
    "                     'max_depth': [random.randint(0, len(rf_max_depths) - 1)],\n",
    "                     'min_samples_leaf': [random.randint(0, len(min_samples) - 1)],\n",
    "                     'criterion': [random.randint(0, len(criterions) - 1)]\n",
    "                    }\n",
    "        \n",
    "        # cross validate over each season\n",
    "        for season in list(X['Season'].unique()):\n",
    "            \n",
    "            # add season to scores dictionary\n",
    "            if season not in scores:\n",
    "                scores[season] = []\n",
    "            \n",
    "            # split into train and validation sets\n",
    "            X_train = X[X['Season'] != season].drop(columns = ['Season'])\n",
    "            X_val = X[X['Season'] == season].drop(columns = ['Season'])\n",
    "            y_train = y[X_train.index]\n",
    "            y_val = y[X_val.index]\n",
    "            \n",
    "            # fit random forest model\n",
    "            rf_model = RandomForestClassifier(n_estimators = 1000,\n",
    "                                             criterion = rf_params['criterion'],\n",
    "                                             max_depth = rf_params['max_depth'],\n",
    "                                             min_samples_leaf = rf_params['min_samples_leaf'],\n",
    "                                             max_features = rf_params['max_features'],\n",
    "                                             random_state = 0).fit(X_train, y_train)\n",
    "            \n",
    "            # predict win probabilities\n",
    "            predictions = rf_model.predict_proba(X_val)\n",
    "            \n",
    "            # calculate log loss and store score\n",
    "            val_score = log_loss(y_val, predictions)\n",
    "            scores[season].append(val_score)\n",
    "            \n",
    "        # store model details\n",
    "        model_coefs.append(rf_params)\n",
    "        models.append(rf_model)\n",
    "        \n",
    "    # return dataframe of results\n",
    "    return pd.DataFrame({'Type': ['rf' for i in range(len(models))],\n",
    "                         'Num_Features': ['All' for i in range(len(models))],\n",
    "                         'Features': ['All' for i in range(len(models))],\n",
    "                         'Model': models,\n",
    "                         'Model_Coef': model_coefs,\n",
    "                         '2008_Score': scores[2008],\n",
    "                         '2009_Score': scores[2009],\n",
    "                         '2010_Score': scores[2010],\n",
    "                         '2011_Score': scores[2011],\n",
    "                         '2012_Score': scores[2012],\n",
    "                         '2013_Score': scores[2013],\n",
    "                         '2014_Score': scores[2014],\n",
    "                         '2015_Score': scores[2015],\n",
    "                         '2016_Score': scores[2016],\n",
    "                         '2017_Score': scores[2017],\n",
    "                         '2018_Score': scores[2018],\n",
    "                         '2019_Score': scores[2019]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost parameters\n",
    "etas = [0.001, 0.003, 0.01, 0.03, 0.1, 0.3]\n",
    "xgb_max_depths = [2, 6, 10, 14, 18]\n",
    "min_child_weights = [0, 1, 3, 6, 10, 14]\n",
    "gammas = [0, 0.01, 0.03, 0.1, 0.3, 1, 3]\n",
    "subsamples = [0.8, 0.9, 1]\n",
    "lambdas = [0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100, 300, 1000, 3000]\n",
    "\n",
    "def train_xgboost():\n",
    "    \n",
    "    # list/dictionaries to store results\n",
    "    scores = {}\n",
    "    models = []\n",
    "    model_coefs = []\n",
    "    \n",
    "    random.seed(0)\n",
    "    \n",
    "    # fit 10 models\n",
    "    for i in tqdm(range(10)):\n",
    "        \n",
    "        # randomly generate XGBoost params\n",
    "        xgb_params = {'eta': [random.randint(0, len(etas) - 1)],\n",
    "                      'max_depth': [random.randint(0, len(xgb_max_depths) - 1)],\n",
    "                      'min_child_weight': [random.randint(0, len(min_child_weights) - 1)],\n",
    "                      'gamma': [random.randint(0, len(gammas) - 1)],\n",
    "                      'subsample': [random.randint(0, len(subsamples) - 1)],\n",
    "                      'lambda': [random.randint(0, len(lambdas) - 1)],\n",
    "                      'random_state': 0\n",
    "                     }\n",
    "        \n",
    "        # cross validate over each season\n",
    "        for season in list(X['Season'].unique()):\n",
    "            \n",
    "            # add season to scores dictionary\n",
    "            if season not in scores:\n",
    "                scores[season] = []\n",
    "                \n",
    "            # split into train and validation sets\n",
    "            X_train = X[X['Season'] != season].drop(columns = ['Season'])\n",
    "            X_val = X[X['Season'] == season].drop(columns = ['Season'])\n",
    "            y_train = y[X_train.index]\n",
    "            y_val = y[X_val.index]\n",
    "            \n",
    "            # fit XGBoost model\n",
    "            xgb_model = XGBClassifier(**xgb_params).fit(X_train, y_train)\n",
    "            \n",
    "            # predict win probabilities\n",
    "            predictions = xgb_model.predict_proba(X_val)\n",
    "            \n",
    "            # calculate log loss and store score\n",
    "            val_score = log_loss(y_val, predictions)\n",
    "            scores[season].append(val_score)\n",
    "            \n",
    "        # store model details\n",
    "        model_coefs.append(xgb_params)\n",
    "        models.append(xgb_model)\n",
    "        \n",
    "    # return dataframe of results\n",
    "    return pd.DataFrame({'Type': ['xgb' for i in range(len(models))],\n",
    "                         'Num_Features': ['All' for i in range(len(models))],\n",
    "                         'Features': ['All' for i in range(len(models))],\n",
    "                         'Model': models,\n",
    "                         'Model_Coef': model_coefs,\n",
    "                         '2008_Score': scores[2008],\n",
    "                         '2009_Score': scores[2009],\n",
    "                         '2010_Score': scores[2010],\n",
    "                         '2011_Score': scores[2011],\n",
    "                         '2012_Score': scores[2012],\n",
    "                         '2013_Score': scores[2013],\n",
    "                         '2014_Score': scores[2014],\n",
    "                         '2015_Score': scores[2015],\n",
    "                         '2016_Score': scores[2016],\n",
    "                         '2017_Score': scores[2017],\n",
    "                         '2018_Score': scores[2018],\n",
    "                         '2019_Score': scores[2019]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the models\n",
    "log_results = train_logistic_regression()\n",
    "rf_results = train_random_forest()\n",
    "xgb_results = train_xgboost()\n",
    "final_results = pd.concat([log_results, rf_results, xgb_results], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate percentiles of scores in relation to other models\n",
    "percentile_cols = []\n",
    "score_cols = []\n",
    "for col in final_results.columns:\n",
    "    if col.endswith(\"Score\"):\n",
    "        score_cols.append(col)\n",
    "        final_results[col[0:4] + \"_Percentile\"] = final_results[col].rank(pct = True, ascending = False)\n",
    "        percentile_cols.append(col[0:4] + \"_Percentile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average score and average percentile\n",
    "final_results['Avg_Score'] = final_results[score_cols].mean(axis = 1)\n",
    "final_results['Avg_Percentile'] = final_results[percentile_cols].mean(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to show feature importances of given model by results row index\n",
    "def features(row_index):\n",
    "    model = final_results.loc[row_index, 'Model']\n",
    "    \n",
    "    # if model is logistic regression, return coefficients\n",
    "    if final_results.loc[row_index, 'Type'] == 'log':\n",
    "        result_string = 'Feature Coefficients: \\n\\n'\n",
    "        \n",
    "        model_features = final_results.loc[row_index, 'Features']\n",
    "        for i in range(len(model_features)):\n",
    "            result_string = result_string + model_features[i] + ': ' + model.coef_[0][i] + '\\n'\n",
    "        print(result_string)\n",
    "        \n",
    "    # if model is random forest or XGBoost, return feature importance plot\n",
    "    else:\n",
    "        feat_importances = pd.Series(model.feature_importances_, index = X.drop(columns = ['Season']).columns)\n",
    "        return feat_importances.nlargest(15).plot(kind = 'barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print results\n",
    "pd.set_option('display.max_rows', None)\n",
    "final_results[['Type', 'Num_Features', 'Features', 'Model', 'Model_Coef', 'Avg_Score', 'Avg_Percentile']].sort_values(by = ['Avg_Score'], ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
