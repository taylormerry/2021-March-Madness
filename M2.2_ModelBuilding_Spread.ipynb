{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "\n",
    "Build logistic regression, random forest, and XGBoost models to predict the probability of an upset in the NCAA tournament\n",
    "\n",
    "Based on data output in M1_DataCleaning.ipynb, model used in M3_Predictions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Season</th>\n",
       "      <th>TeamrankRating_x</th>\n",
       "      <th>TrankRating_x</th>\n",
       "      <th>OE_x</th>\n",
       "      <th>DE_x</th>\n",
       "      <th>Tempo_x</th>\n",
       "      <th>Seed_x</th>\n",
       "      <th>3ptRate_x</th>\n",
       "      <th>Ast%_x</th>\n",
       "      <th>FT%_x</th>\n",
       "      <th>...</th>\n",
       "      <th>xOffyDefFTRateAvg</th>\n",
       "      <th>yOffxDefFTRateAvg</th>\n",
       "      <th>AbsxOffyDefAstDiff</th>\n",
       "      <th>AbsyOffxDefAstDiff</th>\n",
       "      <th>xOffyDefAstAvg</th>\n",
       "      <th>yOffxDefAstAvg</th>\n",
       "      <th>TotalPossVarSum</th>\n",
       "      <th>GameScoreVarSum</th>\n",
       "      <th>TrankNaiveUpsetProbability</th>\n",
       "      <th>TeamrankNaiveUpsetProbability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008.0</td>\n",
       "      <td>28.5</td>\n",
       "      <td>0.960742</td>\n",
       "      <td>117.213494</td>\n",
       "      <td>88.761128</td>\n",
       "      <td>73.7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.390996</td>\n",
       "      <td>0.522826</td>\n",
       "      <td>0.694618</td>\n",
       "      <td>...</td>\n",
       "      <td>0.380141</td>\n",
       "      <td>0.330101</td>\n",
       "      <td>0.011372</td>\n",
       "      <td>0.152258</td>\n",
       "      <td>0.528512</td>\n",
       "      <td>0.552607</td>\n",
       "      <td>342.200873</td>\n",
       "      <td>0.030100</td>\n",
       "      <td>0.011556</td>\n",
       "      <td>0.022079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008.0</td>\n",
       "      <td>28.5</td>\n",
       "      <td>0.960742</td>\n",
       "      <td>117.213494</td>\n",
       "      <td>88.761128</td>\n",
       "      <td>73.7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.390996</td>\n",
       "      <td>0.522826</td>\n",
       "      <td>0.694618</td>\n",
       "      <td>...</td>\n",
       "      <td>0.396759</td>\n",
       "      <td>0.345179</td>\n",
       "      <td>0.004257</td>\n",
       "      <td>0.109056</td>\n",
       "      <td>0.520698</td>\n",
       "      <td>0.531006</td>\n",
       "      <td>295.048054</td>\n",
       "      <td>0.047594</td>\n",
       "      <td>0.312558</td>\n",
       "      <td>0.323143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008.0</td>\n",
       "      <td>23.6</td>\n",
       "      <td>0.927155</td>\n",
       "      <td>115.187217</td>\n",
       "      <td>92.329124</td>\n",
       "      <td>65.8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.371802</td>\n",
       "      <td>0.610714</td>\n",
       "      <td>0.750341</td>\n",
       "      <td>...</td>\n",
       "      <td>0.403042</td>\n",
       "      <td>0.342810</td>\n",
       "      <td>0.092145</td>\n",
       "      <td>0.018604</td>\n",
       "      <td>0.564642</td>\n",
       "      <td>0.594836</td>\n",
       "      <td>269.327900</td>\n",
       "      <td>0.072884</td>\n",
       "      <td>0.459461</td>\n",
       "      <td>0.493502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008.0</td>\n",
       "      <td>32.4</td>\n",
       "      <td>0.981585</td>\n",
       "      <td>120.970641</td>\n",
       "      <td>85.610492</td>\n",
       "      <td>69.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.291796</td>\n",
       "      <td>0.627572</td>\n",
       "      <td>0.707756</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360666</td>\n",
       "      <td>0.341935</td>\n",
       "      <td>0.079363</td>\n",
       "      <td>0.078373</td>\n",
       "      <td>0.587891</td>\n",
       "      <td>0.574597</td>\n",
       "      <td>266.467752</td>\n",
       "      <td>0.058420</td>\n",
       "      <td>0.030979</td>\n",
       "      <td>0.039768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008.0</td>\n",
       "      <td>32.4</td>\n",
       "      <td>0.981585</td>\n",
       "      <td>120.970641</td>\n",
       "      <td>85.610492</td>\n",
       "      <td>69.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.291796</td>\n",
       "      <td>0.627572</td>\n",
       "      <td>0.707756</td>\n",
       "      <td>...</td>\n",
       "      <td>0.350930</td>\n",
       "      <td>0.316380</td>\n",
       "      <td>0.080697</td>\n",
       "      <td>0.071639</td>\n",
       "      <td>0.587224</td>\n",
       "      <td>0.571230</td>\n",
       "      <td>309.871774</td>\n",
       "      <td>0.046553</td>\n",
       "      <td>0.084577</td>\n",
       "      <td>0.069552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 111 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Season  TeamrankRating_x  TrankRating_x        OE_x       DE_x  Tempo_x  \\\n",
       "0  2008.0              28.5       0.960742  117.213494  88.761128     73.7   \n",
       "1  2008.0              28.5       0.960742  117.213494  88.761128     73.7   \n",
       "2  2008.0              23.6       0.927155  115.187217  92.329124     65.8   \n",
       "3  2008.0              32.4       0.981585  120.970641  85.610492     69.5   \n",
       "4  2008.0              32.4       0.981585  120.970641  85.610492     69.5   \n",
       "\n",
       "   Seed_x  3ptRate_x    Ast%_x     FT%_x  ...  xOffyDefFTRateAvg  \\\n",
       "0       2   0.390996  0.522826  0.694618  ...           0.380141   \n",
       "1       2   0.390996  0.522826  0.694618  ...           0.396759   \n",
       "2       3   0.371802  0.610714  0.750341  ...           0.403042   \n",
       "3       1   0.291796  0.627572  0.707756  ...           0.360666   \n",
       "4       1   0.291796  0.627572  0.707756  ...           0.350930   \n",
       "\n",
       "   yOffxDefFTRateAvg  AbsxOffyDefAstDiff  AbsyOffxDefAstDiff  xOffyDefAstAvg  \\\n",
       "0           0.330101            0.011372            0.152258        0.528512   \n",
       "1           0.345179            0.004257            0.109056        0.520698   \n",
       "2           0.342810            0.092145            0.018604        0.564642   \n",
       "3           0.341935            0.079363            0.078373        0.587891   \n",
       "4           0.316380            0.080697            0.071639        0.587224   \n",
       "\n",
       "   yOffxDefAstAvg  TotalPossVarSum  GameScoreVarSum  \\\n",
       "0        0.552607       342.200873         0.030100   \n",
       "1        0.531006       295.048054         0.047594   \n",
       "2        0.594836       269.327900         0.072884   \n",
       "3        0.574597       266.467752         0.058420   \n",
       "4        0.571230       309.871774         0.046553   \n",
       "\n",
       "   TrankNaiveUpsetProbability  TeamrankNaiveUpsetProbability  \n",
       "0                    0.011556                       0.022079  \n",
       "1                    0.312558                       0.323143  \n",
       "2                    0.459461                       0.493502  \n",
       "3                    0.030979                       0.039768  \n",
       "4                    0.084577                       0.069552  \n",
       "\n",
       "[5 rows x 111 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matchup data outputted from M1_DataCleaning.ipynb\n",
    "matchups = pd.read_csv('mydata/mens/matchups.csv')\n",
    "matchups.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = matchups['ScorePerPossDiff']\n",
    "X = matchups.drop(columns = ['Upset', 'ScorePerPossDiff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Num_Features</th>\n",
       "      <th>Features</th>\n",
       "      <th>Model</th>\n",
       "      <th>Model_Coef</th>\n",
       "      <th>2008_Score</th>\n",
       "      <th>2009_Score</th>\n",
       "      <th>2010_Score</th>\n",
       "      <th>2011_Score</th>\n",
       "      <th>2012_Score</th>\n",
       "      <th>2013_Score</th>\n",
       "      <th>2014_Score</th>\n",
       "      <th>2015_Score</th>\n",
       "      <th>2016_Score</th>\n",
       "      <th>2017_Score</th>\n",
       "      <th>2018_Score</th>\n",
       "      <th>2019_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rf</td>\n",
       "      <td>All</td>\n",
       "      <td>All</td>\n",
       "      <td>RandomForestRegressor(max_features=26, min_sam...</td>\n",
       "      <td>{'max_features': 26, 'max_depth': None, 'min_s...</td>\n",
       "      <td>0.036087</td>\n",
       "      <td>0.030588</td>\n",
       "      <td>0.027593</td>\n",
       "      <td>0.036160</td>\n",
       "      <td>0.021032</td>\n",
       "      <td>0.043503</td>\n",
       "      <td>0.030992</td>\n",
       "      <td>0.023487</td>\n",
       "      <td>0.035227</td>\n",
       "      <td>0.027618</td>\n",
       "      <td>0.032460</td>\n",
       "      <td>0.032098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rf</td>\n",
       "      <td>All</td>\n",
       "      <td>All</td>\n",
       "      <td>RandomForestRegressor(max_features=30, min_sam...</td>\n",
       "      <td>{'max_features': 30, 'max_depth': None, 'min_s...</td>\n",
       "      <td>0.034537</td>\n",
       "      <td>0.029874</td>\n",
       "      <td>0.026415</td>\n",
       "      <td>0.034842</td>\n",
       "      <td>0.021782</td>\n",
       "      <td>0.043381</td>\n",
       "      <td>0.032298</td>\n",
       "      <td>0.023499</td>\n",
       "      <td>0.033808</td>\n",
       "      <td>0.026079</td>\n",
       "      <td>0.034656</td>\n",
       "      <td>0.033101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rf</td>\n",
       "      <td>All</td>\n",
       "      <td>All</td>\n",
       "      <td>RandomForestRegressor(max_depth=20, max_featur...</td>\n",
       "      <td>{'max_features': 28, 'max_depth': 20, 'min_sam...</td>\n",
       "      <td>0.034165</td>\n",
       "      <td>0.030089</td>\n",
       "      <td>0.026526</td>\n",
       "      <td>0.034218</td>\n",
       "      <td>0.021494</td>\n",
       "      <td>0.043172</td>\n",
       "      <td>0.032083</td>\n",
       "      <td>0.023230</td>\n",
       "      <td>0.034228</td>\n",
       "      <td>0.025750</td>\n",
       "      <td>0.034560</td>\n",
       "      <td>0.033075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rf</td>\n",
       "      <td>All</td>\n",
       "      <td>All</td>\n",
       "      <td>RandomForestRegressor(max_depth=20, max_featur...</td>\n",
       "      <td>{'max_features': 30, 'max_depth': 20, 'min_sam...</td>\n",
       "      <td>0.036128</td>\n",
       "      <td>0.030680</td>\n",
       "      <td>0.027804</td>\n",
       "      <td>0.036257</td>\n",
       "      <td>0.021156</td>\n",
       "      <td>0.043208</td>\n",
       "      <td>0.030860</td>\n",
       "      <td>0.023331</td>\n",
       "      <td>0.035110</td>\n",
       "      <td>0.027765</td>\n",
       "      <td>0.032487</td>\n",
       "      <td>0.032211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rf</td>\n",
       "      <td>All</td>\n",
       "      <td>All</td>\n",
       "      <td>RandomForestRegressor(max_features=26, min_sam...</td>\n",
       "      <td>{'max_features': 26, 'max_depth': None, 'min_s...</td>\n",
       "      <td>0.039553</td>\n",
       "      <td>0.032409</td>\n",
       "      <td>0.027401</td>\n",
       "      <td>0.039097</td>\n",
       "      <td>0.023398</td>\n",
       "      <td>0.045343</td>\n",
       "      <td>0.032957</td>\n",
       "      <td>0.025358</td>\n",
       "      <td>0.037614</td>\n",
       "      <td>0.031062</td>\n",
       "      <td>0.032193</td>\n",
       "      <td>0.033994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Type Num_Features Features  \\\n",
       "0   rf          All      All   \n",
       "1   rf          All      All   \n",
       "2   rf          All      All   \n",
       "3   rf          All      All   \n",
       "4   rf          All      All   \n",
       "\n",
       "                                               Model  \\\n",
       "0  RandomForestRegressor(max_features=26, min_sam...   \n",
       "1  RandomForestRegressor(max_features=30, min_sam...   \n",
       "2  RandomForestRegressor(max_depth=20, max_featur...   \n",
       "3  RandomForestRegressor(max_depth=20, max_featur...   \n",
       "4  RandomForestRegressor(max_features=26, min_sam...   \n",
       "\n",
       "                                          Model_Coef  2008_Score  2009_Score  \\\n",
       "0  {'max_features': 26, 'max_depth': None, 'min_s...    0.036087    0.030588   \n",
       "1  {'max_features': 30, 'max_depth': None, 'min_s...    0.034537    0.029874   \n",
       "2  {'max_features': 28, 'max_depth': 20, 'min_sam...    0.034165    0.030089   \n",
       "3  {'max_features': 30, 'max_depth': 20, 'min_sam...    0.036128    0.030680   \n",
       "4  {'max_features': 26, 'max_depth': None, 'min_s...    0.039553    0.032409   \n",
       "\n",
       "   2010_Score  2011_Score  2012_Score  2013_Score  2014_Score  2015_Score  \\\n",
       "0    0.027593    0.036160    0.021032    0.043503    0.030992    0.023487   \n",
       "1    0.026415    0.034842    0.021782    0.043381    0.032298    0.023499   \n",
       "2    0.026526    0.034218    0.021494    0.043172    0.032083    0.023230   \n",
       "3    0.027804    0.036257    0.021156    0.043208    0.030860    0.023331   \n",
       "4    0.027401    0.039097    0.023398    0.045343    0.032957    0.025358   \n",
       "\n",
       "   2016_Score  2017_Score  2018_Score  2019_Score  \n",
       "0    0.035227    0.027618    0.032460    0.032098  \n",
       "1    0.033808    0.026079    0.034656    0.033101  \n",
       "2    0.034228    0.025750    0.034560    0.033075  \n",
       "3    0.035110    0.027765    0.032487    0.032211  \n",
       "4    0.037614    0.031062    0.032193    0.033994  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in previous results to not duplicate params that have already been tested\n",
    "prev_results = pd.read_csv('mydata/mens/training_results_spread.csv')\n",
    "prev_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lasso regularization for feature selection\n",
    "# coefficients range from 0.000001 to 0.1\n",
    "# each coefficient is 12.2% larger than last, there are 100 coefficients in total\n",
    "lasso_reg = [1e-6 * (1.122) ** i for i in range(100)]\n",
    "\n",
    "# ridge regularization to prevent overfitting\n",
    "ridge_reg = [1e-7, 3e-7, 1e-6, 3e-6, 1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3, 1e-2, 3e-2, 1e-1, 1e0, 3e0, 1e1, 3e1, 1e2, 3e2, 1e3, 3e3, 1e4, 3e4, 1e5, 3e5, 1e6, 3e6, 1e7, 3e7]\n",
    "\n",
    "# scale used to scale columns before applying regularization\n",
    "scale = StandardScaler()\n",
    "\n",
    "def train_linear_regression():\n",
    "    \n",
    "    # list/dictionaries to store results\n",
    "    feature_list = []\n",
    "    scores = {}\n",
    "    model_coefs = []\n",
    "    models = []\n",
    "    num_features = []\n",
    "    \n",
    "    # for each possible lasso regularization coefficient\n",
    "    for c in tqdm(lasso_reg):\n",
    "        \n",
    "        # dont use season as a feature\n",
    "        X_lasso = X.drop(columns = ['Season'])\n",
    "        \n",
    "        # scale columns before regularization\n",
    "        X_lasso = pd.DataFrame(scale.fit_transform(X_lasso), columns = X_lasso.columns)\n",
    "        \n",
    "        # fit L1 logistic regression for feature selection\n",
    "        lasso = Lasso(alpha = c, random_state = 0, max_iter = 10000).fit(X_lasso, y)\n",
    "        \n",
    "        # filter for the columns with nonzero coefficients and the season column\n",
    "        zero_cols = []\n",
    "        for i in range(len(lasso.coef_)):\n",
    "            if lasso.coef_[i] == 0.0:\n",
    "                zero_cols.append(X_lasso.columns[i])\n",
    "        nonzero_X = X.drop(columns = zero_cols)\n",
    "        \n",
    "        # if there is at least 1 nonzero column and the same amount of features hasn't already been built\n",
    "        if (len(nonzero_X.columns) > 1 and (len(nonzero_X.columns) - 1) not in num_features):\n",
    "            \n",
    "            # try each L2 regularization coefficient\n",
    "            for c2 in ridge_reg:\n",
    "                \n",
    "                # cross validate over each season\n",
    "                for season in list(X['Season'].unique()):\n",
    "                    \n",
    "                    # add season to scores dictionary\n",
    "                    if season not in scores:\n",
    "                        scores[season] = []\n",
    "                        \n",
    "                    # split into train and validation sets\n",
    "                    X_train = nonzero_X[nonzero_X['Season'] != season].drop(columns = ['Season'])\n",
    "                    X_val = nonzero_X[nonzero_X['Season'] == season].drop(columns = ['Season'])\n",
    "                    X_train = pd.DataFrame(scale.fit_transform(X_train), columns = X_train.columns)\n",
    "                    X_val = pd.DataFrame(scale.transform(X_val), columns = X_val.columns)\n",
    "                    y_train = y[X_train.index]\n",
    "                    y_val = y[X_val.index]\n",
    "                    \n",
    "                    # fit logistic regression\n",
    "                    lin_model = Ridge(alpha = c2, max_iter = 10000, random_state = 0).fit(X_train, y_train)\n",
    "                    \n",
    "                    # predict win probabilities\n",
    "                    predictions = lin_model.predict(X_val)\n",
    "                    \n",
    "                    # calculate log loss and store\n",
    "                    val_score = mean_squared_error(y_val, predictions)\n",
    "                    scores[season].append(val_score)\n",
    "                    \n",
    "                # retrain model on full dataset for coefficients\n",
    "                lin_model = Ridge(alpha = c2, max_iter = 10000, random_state = 0).fit(pd.DataFrame(scale.fit_transform(nonzero_X), columns = nonzero_X.columns).drop(columns = ['Season']), y)\n",
    "                \n",
    "                # store model details\n",
    "                feature_list.append(nonzero_X.drop(columns = ['Season']).columns)\n",
    "                num_features.append(len(nonzero_X.drop(columns = ['Season']).columns))\n",
    "                models.append(lin_model)\n",
    "                model_coefs.append({'lasso_coef': c, 'ridge_coef': c2})\n",
    "                \n",
    "    # return dataframe of results\n",
    "    return pd.DataFrame({'Type': ['log' for i in range(len(models))],\n",
    "                         'Num_Features': num_features,\n",
    "                         'Features': feature_list,\n",
    "                         'Model': models,\n",
    "                         'Model_Coef': model_coefs,\n",
    "                         '2008_Score': scores[2008],\n",
    "                         '2009_Score': scores[2009],\n",
    "                         '2010_Score': scores[2010],\n",
    "                         '2011_Score': scores[2011],\n",
    "                         '2012_Score': scores[2012],\n",
    "                         '2013_Score': scores[2013],\n",
    "                         '2014_Score': scores[2014],\n",
    "                         '2015_Score': scores[2015],\n",
    "                         '2016_Score': scores[2016],\n",
    "                         '2017_Score': scores[2017],\n",
    "                         '2018_Score': scores[2018],\n",
    "                         '2019_Score': scores[2019]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest paramaters\n",
    "max_features_list = [27, 28, 29, 30, 31, 32, 33]\n",
    "rf_max_depths = [1, 2, 3, 4]\n",
    "min_samples = [0.001, 0.003, 0.01]\n",
    "\n",
    "def train_random_forest():\n",
    "    \n",
    "    # list/dictionaries to store results\n",
    "    scores = {}\n",
    "    models = []\n",
    "    model_coefs = []\n",
    "    \n",
    "    # fit 100 models\n",
    "    for i in tqdm(range(1)):\n",
    "        \n",
    "        # randomly generate random forest params\n",
    "        rf_params = {'max_features': max_features_list[random.randint(0, len(max_features_list) - 1)],\n",
    "                     'max_depth': rf_max_depths[random.randint(0, len(rf_max_depths) - 1)],\n",
    "                     'min_samples_leaf': min_samples[random.randint(0, len(min_samples) - 1)]\n",
    "                    }\n",
    "        \n",
    "        # keep generating params if they've already been tested\n",
    "        while rf_params in list(prev_results['Model_Coef']):\n",
    "            rf_params = {'max_features': max_features_list[random.randint(0, len(max_features_list) - 1)],\n",
    "                         'max_depth': rf_max_depths[random.randint(0, len(rf_max_depths) - 1)],\n",
    "                         'min_samples_leaf': min_samples[random.randint(0, len(min_samples) - 1)]\n",
    "                        }\n",
    "        \n",
    "        # cross validate over each season\n",
    "        for season in list(X['Season'].unique()):\n",
    "            \n",
    "            # add season to scores dictionary\n",
    "            if season not in scores:\n",
    "                scores[season] = []\n",
    "            \n",
    "            # split into train and validation sets\n",
    "            X_train = X[X['Season'] != season].drop(columns = ['Season'])\n",
    "            X_val = X[X['Season'] == season].drop(columns = ['Season'])\n",
    "            y_train = y[X_train.index]\n",
    "            y_val = y[X_val.index]\n",
    "            \n",
    "            # fit random forest model\n",
    "            rf_model = RandomForestRegressor(n_estimators = 500,\n",
    "                                             max_depth = rf_params['max_depth'],\n",
    "                                             min_samples_leaf = rf_params['min_samples_leaf'],\n",
    "                                             max_features = rf_params['max_features'],\n",
    "                                             random_state = 0).fit(X_train, y_train)\n",
    "            \n",
    "            # predict win probabilities\n",
    "            predictions = rf_model.predict(X_val)\n",
    "            \n",
    "            # calculate log loss and store score\n",
    "            val_score = mean_squared_error(y_val, predictions)\n",
    "            scores[season].append(val_score)\n",
    "            \n",
    "        # retrain model on full dataset for feature importances\n",
    "        rf_model = RandomForestRegressor(n_estimators = 500,\n",
    "                                         max_depth = rf_params['max_depth'],\n",
    "                                         min_samples_leaf = rf_params['min_samples_leaf'],\n",
    "                                         max_features = rf_params['max_features'],\n",
    "                                         random_state = 0).fit(X.drop(columns = ['Season']), y)\n",
    "        \n",
    "        # store model details\n",
    "        model_coefs.append(rf_params)\n",
    "        models.append(rf_model)\n",
    "        \n",
    "    # return dataframe of results\n",
    "    return pd.DataFrame({'Type': ['rf' for i in range(len(models))],\n",
    "                         'Num_Features': ['All' for i in range(len(models))],\n",
    "                         'Features': ['All' for i in range(len(models))],\n",
    "                         'Model': models,\n",
    "                         'Model_Coef': model_coefs,\n",
    "                         '2008_Score': scores[2008],\n",
    "                         '2009_Score': scores[2009],\n",
    "                         '2010_Score': scores[2010],\n",
    "                         '2011_Score': scores[2011],\n",
    "                         '2012_Score': scores[2012],\n",
    "                         '2013_Score': scores[2013],\n",
    "                         '2014_Score': scores[2014],\n",
    "                         '2015_Score': scores[2015],\n",
    "                         '2016_Score': scores[2016],\n",
    "                         '2017_Score': scores[2017],\n",
    "                         '2018_Score': scores[2018],\n",
    "                         '2019_Score': scores[2019]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost parameters\n",
    "etas = [0.01, 0.03, 0.1, 0.3, 1, 3, 10]\n",
    "xgb_max_depths = [9, 10, 11, 12, 13, 14]\n",
    "min_child_weights = [6]\n",
    "gammas = [0.3]\n",
    "subsamples = [1]\n",
    "lambdas = [0.1, 0.3, 1, 3, 10, 30, 100, 300, 1000, 3000]\n",
    "\n",
    "def train_xgboost():\n",
    "    \n",
    "    # list/dictionaries to store results\n",
    "    scores = {}\n",
    "    models = []\n",
    "    model_coefs = []\n",
    "    \n",
    "    # fit 100 models\n",
    "    for i in tqdm(range(50)):\n",
    "        \n",
    "        # randomly generate XGBoost params\n",
    "        xgb_params = {'eta': etas[random.randint(0, len(etas) - 1)],\n",
    "                      'max_depth': xgb_max_depths[random.randint(0, len(xgb_max_depths) - 1)],\n",
    "                      'min_child_weight': min_child_weights[random.randint(0, len(min_child_weights) - 1)],\n",
    "                      'gamma': gammas[random.randint(0, len(gammas) - 1)],\n",
    "                      'subsample': subsamples[random.randint(0, len(subsamples) - 1)],\n",
    "                      'lambda': lambdas[random.randint(0, len(lambdas) - 1)],\n",
    "                      'random_state': 0\n",
    "                     }\n",
    "        # keep randomly generating if params have already been tested\n",
    "        while xgb_params in list(prev_results['Model_Coef']):\n",
    "            # randomly generate XGBoost params\n",
    "            xgb_params = {'eta': etas[random.randint(0, len(etas) - 1)],\n",
    "                          'max_depth': xgb_max_depths[random.randint(0, len(xgb_max_depths) - 1)],\n",
    "                          'min_child_weight': min_child_weights[random.randint(0, len(min_child_weights) - 1)],\n",
    "                          'gamma': gammas[random.randint(0, len(gammas) - 1)],\n",
    "                          'subsample': subsamples[random.randint(0, len(subsamples) - 1)],\n",
    "                          'lambda': lambdas[random.randint(0, len(lambdas) - 1)],\n",
    "                          'random_state': 0\n",
    "                         }\n",
    "        \n",
    "        # cross validate over each season\n",
    "        for season in list(X['Season'].unique()):\n",
    "            \n",
    "            # add season to scores dictionary\n",
    "            if season not in scores:\n",
    "                scores[season] = []\n",
    "                \n",
    "            # split into train and validation sets\n",
    "            X_train = X[X['Season'] != season].drop(columns = ['Season'])\n",
    "            X_val = X[X['Season'] == season].drop(columns = ['Season'])\n",
    "            y_train = y[X_train.index]\n",
    "            y_val = y[X_val.index]\n",
    "            \n",
    "            # fit XGBoost model\n",
    "            xgb_model = XGBRegressor(**xgb_params).fit(X_train, y_train)\n",
    "            \n",
    "            # predict win probabilities\n",
    "            predictions = xgb_model.predict(X_val)\n",
    "            \n",
    "            # calculate log loss and store score\n",
    "            val_score = mean_squared_error(y_val, predictions)\n",
    "            scores[season].append(val_score)\n",
    "            \n",
    "        # retrain model on full dataset for feature importances\n",
    "        xgb_model = XGBRegressor(**xgb_params).fit(X.drop(columns = ['Season']), y)\n",
    "        \n",
    "        # store model details\n",
    "        model_coefs.append(xgb_params)\n",
    "        models.append(xgb_model)\n",
    "        \n",
    "    # return dataframe of results\n",
    "    return pd.DataFrame({'Type': ['xgb' for i in range(len(models))],\n",
    "                         'Num_Features': ['All' for i in range(len(models))],\n",
    "                         'Features': ['All' for i in range(len(models))],\n",
    "                         'Model': models,\n",
    "                         'Model_Coef': model_coefs,\n",
    "                         '2008_Score': scores[2008],\n",
    "                         '2009_Score': scores[2009],\n",
    "                         '2010_Score': scores[2010],\n",
    "                         '2011_Score': scores[2011],\n",
    "                         '2012_Score': scores[2012],\n",
    "                         '2013_Score': scores[2013],\n",
    "                         '2014_Score': scores[2014],\n",
    "                         '2015_Score': scores[2015],\n",
    "                         '2016_Score': scores[2016],\n",
    "                         '2017_Score': scores[2017],\n",
    "                         '2018_Score': scores[2018],\n",
    "                         '2019_Score': scores[2019]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:21<00:00, 21.41s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [16:05<00:00, 19.31s/it]\n"
     ]
    }
   ],
   "source": [
    "# train the models\n",
    "#lm_results = train_linear_regression()\n",
    "rf_results = train_random_forest()\n",
    "xgb_results = train_xgboost()\n",
    "final_results = pd.concat([prev_results, rf_results, xgb_results], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate percentiles of scores in relation to other models\n",
    "score_cols = []\n",
    "for col in final_results.columns:\n",
    "    if col.endswith(\"Score\"):\n",
    "        score_cols.append(col)\n",
    "\n",
    "# calculate average score and average percentile\n",
    "final_results['Avg_Score'] = final_results[score_cols].mean(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to show feature importances of given model by results row index\n",
    "def features(row_index):\n",
    "    model = final_results.loc[row_index, 'Model']\n",
    "    \n",
    "    # if model is logistic regression, return coefficients\n",
    "    if final_results.loc[row_index, 'Type'] == 'log':\n",
    "        model_features = list(final_results.loc[row_index, 'Features'])\n",
    "        sign_list = []\n",
    "        coef_list = []\n",
    "        for i in range(len(model_features)):\n",
    "            coef = round(model.coef_[i], 6)\n",
    "            coef_list.append(coef)\n",
    "            if coef > 0:\n",
    "                sign_list.append('+')\n",
    "            else:\n",
    "                sign_list.append('-')\n",
    "        \n",
    "        return_df =  pd.DataFrame({'Feature': model_features,\n",
    "                             'Sign': sign_list,\n",
    "                            'Coefficient': coef_list})\n",
    "        return return_df.reindex(return_df['Coefficient'].abs().sort_values(ascending = False).index)\n",
    "        \n",
    "    # if model is random forest or XGBoost, return feature importance plot\n",
    "    else:\n",
    "        feat_importances = pd.Series(model.feature_importances_, index = X.drop(columns = ['Season']).columns)\n",
    "        return feat_importances.nlargest(15).plot(kind = 'barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to show params of given model by results row index\n",
    "def params(row_index):\n",
    "    return final_results.loc[row_index, 'Model_Coef']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Avg_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>770</th>\n",
       "      <td>rf</td>\n",
       "      <td>0.030644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>783</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>xgb</td>\n",
       "      <td>0.030817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Type  Avg_Score\n",
       "770   rf   0.030644\n",
       "720  xgb   0.030783\n",
       "781  xgb   0.030783\n",
       "809  xgb   0.030783\n",
       "808  xgb   0.030783\n",
       "782  xgb   0.030783\n",
       "783  xgb   0.030783\n",
       "805  xgb   0.030783\n",
       "785  xgb   0.030783\n",
       "787  xgb   0.030783\n",
       "788  xgb   0.030783\n",
       "789  xgb   0.030783\n",
       "790  xgb   0.030783\n",
       "793  xgb   0.030783\n",
       "799  xgb   0.030783\n",
       "732  xgb   0.030783\n",
       "810  xgb   0.030783\n",
       "819  xgb   0.030783\n",
       "779  xgb   0.030783\n",
       "738  xgb   0.030783\n",
       "740  xgb   0.030783\n",
       "745  xgb   0.030783\n",
       "758  xgb   0.030783\n",
       "811  xgb   0.030783\n",
       "815  xgb   0.030783\n",
       "814  xgb   0.030783\n",
       "798  xgb   0.030783\n",
       "772  xgb   0.030783\n",
       "774  xgb   0.030783\n",
       "775  xgb   0.030783\n",
       "776  xgb   0.030783\n",
       "812  xgb   0.030783\n",
       "633  xgb   0.030814\n",
       "649  xgb   0.030814\n",
       "796  xgb   0.030814\n",
       "588  xgb   0.030814\n",
       "733  xgb   0.030814\n",
       "648  xgb   0.030814\n",
       "663  xgb   0.030814\n",
       "820  xgb   0.030814\n",
       "533  xgb   0.030814\n",
       "813  xgb   0.030814\n",
       "405  xgb   0.030814\n",
       "807  xgb   0.030814\n",
       "797  xgb   0.030817\n",
       "773  xgb   0.030817\n",
       "816  xgb   0.030817\n",
       "803  xgb   0.030817\n",
       "795  xgb   0.030817\n",
       "777  xgb   0.030817"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print results\n",
    "pd.set_option('display.max_rows', 50)\n",
    "final_results[['Type', 'Avg_Score']].sort_values(by = ['Avg_Score'], ascending = True).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 33, 'max_depth': 2, 'min_samples_leaf': 0.01}\n",
      "{'max_features': 33, 'max_depth': 2, 'min_samples_leaf': 0.003}\n"
     ]
    }
   ],
   "source": [
    "for i in [781, 794]:\n",
    "    print(params(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "features(809)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(eta=0.03, gamma=0.3, lambda=30, max_depth=11, min_child_weight=6)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results.loc[809, 'Model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results[['Type', 'Num_Features', 'Features', 'Model', 'Model_Coef', '2008_Score',\n",
    "               '2009_Score', '2010_Score', '2011_Score', '2012_Score', '2013_Score',\n",
    "               '2014_Score', '2015_Score', '2016_Score', '2017_Score', '2018_Score',\n",
    "               '2019_Score']].to_csv('mydata/training_results_spread.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(final_results.loc[781, 'Model'], open('models/MSpreadRF.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(final_results.loc[809, 'Model'], open('models/MSpreadXGB.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
